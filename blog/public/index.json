[{"content":"\u0026ldquo;Leave this ecosystem? No way! Why should I leave it when they are providing everything for free and it\u0026rsquo;s so convenient when everything is just there when you want it!\u0026rdquo; Yep! This is me approximately one year ago when I was heavily tied into the Google Ecosystem and I had been totally involved in all aspects inside this Ecosystem for five to six years straight. And it took me couple of months to actually explore the internet and discover why it was a bad idea. It\u0026rsquo;s very easy to say that any service on the internet is convenient because it makes your life so easier. However, every convenience comes with a cost. In here, your freedom of choice and your privacy is at stake!\nThe Problem I didn\u0026rsquo;t find the idea of moving out of my ecosystem was considerable because my Ecosystem seemed like my \u0026lsquo;Eden Garden\u0026rsquo; where everything I wanted to do was in my hand\u0026rsquo;s reach. Want to store and retrieve photos easily? Use Google Photos. Want to store your documents? Use Google Drive. Want to edit those documents? Use Google docs. Want to add your photos to those documents? Google photos is integrated with Google docs. It\u0026rsquo;s as if the your Ecosystem knows what you want to do even before you even think of doing it. This nature of an Ecosystem poses us with two big problems.\n Privacy : Your Ecosystem knows a lot about you already to understand what you would possibly want to do (Only if it\u0026rsquo;s collecting your data which is what most of the ecosystems do). Restriction : Your Ecosystem confines you inside its walls restricting you from even thinking about migrating from your services because if you do, you may have to give up some of that convenience. And it\u0026rsquo;s gonna take a lot of time to unhook the hooks the ecosystem puts on you.  Privacy in an Ecosystem is a bigger discussion in itself. I want to specifically talk about the second one where it confines you inside its walls, which had happened to me. Convenience is like a drug (to put it bluntly). And even if I had known that my ecosystem tracks me 24/7, I was reluctant to give away my convenience. When you are heavily tied into an Ecosystem, you stop looking for other services which are clearly better because whatever your Ecosystem provides is the one that would work best for you in the current state and even if it\u0026rsquo;s something crappy and something you didn\u0026rsquo;t like, you would have to live with it. This is how the Ecosystem suppresses your freedom of choice by making you enslaved for what you are being provided, not for what you have demanded. That is basically monopolistic nature put into digital media.\nThe Realization Last year, I chose to read \u0026lsquo;Permanent record\u0026rsquo; by Edward Snowden. It really changed my outlook and it made me question my online presence. It was really implicative when I looked back at my \u0026ldquo;Ecosystematical\u0026rdquo; lifestyle and how much I was missing out on the true essence of what makes the internet special. And, I decided I would migrate my data to those services that are clearly better and explore the wide world out there. And the journey was painfully slow because I had an archive of 50+ GB\u0026rsquo;s of photos in Google Photos and most of my documents were backed up in Google Drive.\nThe Takeout Process Google has a takeout program where you can select all of your data and then request for a copy. Google then aggregates your data and then compresses them down to a zip file. You\u0026rsquo;ll get an email in your gmail inbox. Then you can download your data through the link. Easy right? No!!\nWhat Google does, is it lets you choose how big of a data chunk you want to download. Say you have an archive of 20GB, you can download it in two chunks of 10GB each or 10 chunks of 2GB each. And those downloads will fail if your internet connection drops down certain threshold. Then you need to retry. If it fails more than 3 times, you need to request for the data again. Most of the people would give up here and choose to spend some more time in this ecosystem until they try again and the cycle repeats.\nI had to request for my data thrice and I still couldn\u0026rsquo;t download all my data. And the only inconvinient process around the whole ecosystem was the takeout process. I don\u0026rsquo;t know if it was done deliberately, but I had to let go of my 50GB+ of archive which had very important documents and photos. My Eden Garden was punishing me when I decided to move away from it.\nThe Solution The solution is to move towards open source projects or towards the services that focus on your privacy. When any service focuses on your privacy, it just means they value you as an end user. Internet means Freedom. So, you should be the one who own your free will to navigate around and call yourself at home. It sure takes a lot of trial and errors to make yourself comfortable by finding a balance between convenicence and digital privacy. But it\u0026rsquo;s gonna be worth it. The more private you go, less convinient your experience will be. However, there are so many alternatives on the world wide web which is enough for you to build your own convenient ecosystem.\nYou might want to look at NextCloud Project which is a self hosted cloud solution which has plugins for any use case that you might have.\nIf you want to start self hosting cheap and best, you might want to look at Raspberry Pi Project which is a credit card sized computer which can be used to host your self hosted setup which can be scaled further in future. The possibilities are endless.\nSo, Instead of living in someone else\u0026rsquo;s Eden Garden where you live in their terms, why don\u0026rsquo;t you make your own Garden and live by your terms. Isn\u0026rsquo;t that what freedom tastes like?\n","permalink":"https://0xv31an5a1i5.xyz/posts/ecosystem-incarceration/","summary":"\u0026ldquo;Leave this ecosystem? No way! Why should I leave it when they are providing everything for free and it\u0026rsquo;s so convenient when everything is just there when you want it!\u0026rdquo; Yep! This is me approximately one year ago when I was heavily tied into the Google Ecosystem and I had been totally involved in all aspects inside this Ecosystem for five to six years straight. And it took me couple of months to actually explore the internet and discover why it was a bad idea.","title":"Ecosystematic Walls of Incarceration "},{"content":"I have been using Koa.js for a while now and it turns out to be great. I‚Äôm having a lot of fun playing around with it since its simple and configurable. And I was searching for a ground up way to execute file uploads so that I can tweak its functionality however I want to. And it turns out, It‚Äôs not as difficult as I thought. Before going forward, I‚Äôm assuming you have a basic knowledge of how to setup basic Koa.js server with routers since it‚Äôs being covered in so many articles. So I‚Äôll be skipping the server setup with routers. Click here if you want to go through an article that covers the same. So, Lets get coding. üòâ\nServer Page : app.js // app.js const Koa = require(\u0026quot;koa\u0026quot;); const koaBody = require(\u0026quot;koa-body\u0026quot;); const logger = require(\u0026quot;koa-logger\u0026quot;); const router = require(\u0026quot;./router\u0026quot;); const app = new Koa(); app.use(koaBody( { multipart: true } )); app.use(logger()); app.use(router.routes()).use(router.allowedMethods()); app.listen(3000, () =\u0026gt; { console.log(\u0026quot;Listening at 3000\u0026quot;); }); The above app.js file starts up the server and starts listening in port 3000 for requests. Simple stuff, nothing fancy. But things to be noted are the koa-body module and { multipart : true } option. This part is really important, since koa-body parses the request body and populates Koa ctx object based on the form data being sent. When a file is being sent from the front-end, koa-body parses the request and the uploaded file attributes will be available under ctx.request.files which we can then access across Koa server to implement file upload functionality\nRouter and Upload Logic : Router.js Well, Its not a good practice to add the server logic under router.js file since router is only used for routing the requests. You will have a controller file to take care of the logic when the request is being routed from router. But for the course of this tutorial, I have written the logic in the router itself.\nIn this part, you will need a package called promisepipe which I‚Äôll explain in a second. For now, install the package by running the following command. And create a folder named uploads in the root of the project which will hold uploaded images.\nnpm install promisepipe // router.js const Koa = require(\u0026quot;koa\u0026quot;); const Router = require(\u0026quot;koa-router\u0026quot;); const promisePipe = require(\u0026quot;promisepipe\u0026quot;); const fs = require(\u0026quot;fs\u0026quot;); const path = require(\u0026quot;path\u0026quot;); const router = new Router(); router.get(\u0026quot;/\u0026quot;, (context, next) =\u0026gt; { context.body = \u0026quot;Hey\u0026quot;; }); router.post(\u0026quot;/upload\u0026quot;, async (context, next) { try { const uploadfile = context.request.files.file; const savefile = `${Date.now()}#${uploadfile.name}`; const readStream = fs.createReadStream(uploadfile.path); const writeStream = fs.createWriteStream(path.join(\u0026quot;uploads\u0026quot;, savefile)); await promisePipe( readStream.on(\u0026quot;error\u0026quot;, () =\u0026gt; { throw new Error({ errors: \u0026quot;File Read Error\u0026quot; }); }), writeStream.on(\u0026quot;error\u0026quot;, () =\u0026gt; { throw new Error({ errors: \u0026quot;Write Error\u0026quot; }); }) ); context.body = { message: \u0026quot;File Uploaded\u0026quot; }; } catch (err) { console.log(err); context.body = { message: \u0026quot;There was an error\u0026quot;, errors: err }; } }); module.exports = router; The Upload Logic Anything that is written in the bold letters in router.js is important and I‚Äôll be explaining the mode of execution one by one.\n  /upload : This is the POST route handler that contains the logic of what needs to be done when the front-end makes an upload request with a file attached to its body. When the URL is hit, the function gets executed.\n  uploadfile : File attributes from parsed context.request.files.file in uploadfile stored in this variable so that its easier to access it later on in the code. In simple words, uploadfile contains the reference to the file to be uploaded to the server.\n  savefile : The file name to be stored in the folder has to be unique since the same name will be stored in the database to trace down the files from the folder later. So we use {Date.now()}#${uploadfile.nameto create unique name (where uploadfile.name is the name of the file to be uploaded). Which will give us a string of 1560234246152#anyfile.txt for example. Since we add the timstamp, filename stays unique and it will be stored in savefile variable.\n  readStream/writestream : Stream is nothing but a flow of bits from one end to another end. In here, we create a read stream from user‚Äôs local computer and write stream to server‚Äôs uploads directory. We pass uploadfile.path which contains user‚Äôs local file‚Äôs path and uploads folder to write stream. That sets up the stream needed for the upload.\n  promisePipe : promisePipe is a package that takes in two streams ( read / write ) and starts reading from read file to the write file. In here, bits from the user‚Äôs local computer file will be written in streams to the uploads directory on the server with the name mentioned in the variable savefile . The best thing about promisepipe is it returns a promise. It returns a resolved response when the uploading is done or returns the rejected response when there is an error. So we can await untill the uploading is done and continue executing code thereafter, or we can use try/catch block to catch any errors while uploading and handle it.\n  By the end of execution, file will be saved in uploads folder and its name will be stored in savefile variable which can be added to the database for tracking down the files when they are requested. And if there is an error while reading or writing, it will be caught by the catch block and valid response to the front-end will be sent.\nSumming it all up Koa.js is an amazing lightweight configurable framework for Node.js. And arguably, There could be tons of better ways to achieve the same results but this turned out to be the best one for me where i can configure the functionality from the ground up. So if you know of any way to make this code better with added functionalities, I‚Äôd look forward to hear it from you. Happy coding. Have a nice day üòÑ\nImportant Links   Koa.js : Koa is a new web framework designed by the team behind Express, which aims to be a smaller, more expressive, and more. https://koajs.com/\n  koa-body : A Koa body parser middleware. Supports multipart, urlencoded and JSON request bodies. https://www.npmjs.com/package/koa-body\n  promisepipe : Pipe node.js streams safely with Promises. https://www.npmjs.com/package/promisepipe\n  ","permalink":"https://0xv31an5a1i5.xyz/posts/file-upload-koa-js/","summary":"I have been using Koa.js for a while now and it turns out to be great. I‚Äôm having a lot of fun playing around with it since its simple and configurable. And I was searching for a ground up way to execute file uploads so that I can tweak its functionality however I want to. And it turns out, It‚Äôs not as difficult as I thought. Before going forward, I‚Äôm assuming you have a basic knowledge of how to setup basic Koa.","title":"Simple file upload using Koa.js (or Node.js in General)"},{"content":"Docker! Sounds fancy and simple but trust me it has revolutionized how the tech industry builds, ships and deploys the applications lately. Docker is basically a mini-operating system where you build and run your applications which is totally isolated from the native operating system in your computer, and the same container can be deployed in the hosting platforms. If you are new to docker and want to learn more about its intricacies, i highly encourage you to go through the links given in the end of this article to learn more about it. So let‚Äôs get right in :)\nCreating a Dockerfile Dockerfile is the recipe of an image. Dockerfile specifies how your image should look like. However, we are not creating an image from the scratch. We will take the existing image and add our dependencies on top of that to create a customized image based on our requirements. You can setup the Dockerfile to either take the existing requirements.txt file and install dependencies from pip or install dependencies manually without requirements.txt. This is how the Dockerfile will look like in either of the situations.\n FROM python:3 RUN mkdir /usr/src/app WORKDIR /usr/src/app # Below two steps are valid only if you have # requirements.txt in the project folder COPY requirements.txt . RUN pip install -r requirements.txt # If there is no requirements.txt, # You can install individual packages as follows # RUN pip install \u0026lt;package-1\u0026gt; \u0026lt;package-2\u0026gt;...\u0026lt;package-n\u0026gt;   FROM: The image that we are building has to be built in such a way that we take an existing image and add our own functionalities to it. So we are choosing an image with python 3.x since we are dealing with Python developement. However, You can choose any image or any version you want.\n  RUN: This will run a command inside the image. Since our image is a mini linux OS (usually debian distribution) we can run linux commands inside it. Here we run mkdir /usr/src/app which creates a working directory for our application to run. Which is not absolutely necessary since its automatically created in the next step. This step was a demonstration of how to run commands inside an image.\n  WORKDIR: This specifies the working directory of our image. After setting the WORKDIR, Any RUN, CMD, ENTRYPOINT, COPY and ADD commands we execute, will be executed inside the WORKDIR . In our case, its /usr/src/app .\n  COPY: This command will copy any file specified on the left before the space to the directory specified on the right of the space. In our case, we are copying requirements.txt from our project folder on the host to our WORKDIR /usr/src/app in the image And running pip install -r requirements.txt on /usr/src/app which will install and save all the pip dependencies inside the image.\n  If there is no requirements.txt, you can skip the COPY step and install pip dependencies by specifying the pip install command in the RUN section one after the other. And save the Dockerfile with the name Dockerfile\nWe successfully created a recipe of how our image should look like, now the next step is to build the image based on this recipe.\nBuilding from the Dockerfile Now since we are done with creating our recipe, we need to build our own image from the recipe. To build the image, the command is as follows\ndocker build -t python/pack .  docker build is the command that tells docker that we need to build our own image. -t python/pack is where we add our own name for the new docker image. we can also add tags to the image using -t python/pack:latest where latest is the tag for the image. (although latest is a default tag for any new image) . tells the docker to use the current directory in lookout for Dockerfile . If the Dockerfile lies in different directory, you can specify the path from where you would want to build the Dockerfile . Once the build is finished, you will have a Customized docker image based on your recipe, and you can check it by running docker images command in the shell. You should see an image called python/pack\nCreating a Container Containers are the place where we execute our programs by mounting our local volumes to the container volumes. From where you can access the packages and dependencies installed in the container meanwhile, persisting the data to the host system. Run the following command in bash\ndocker run --rm -it \\ -u $(id -u):$(id -g) \\ -v /etc/passwd:/etc/passwd \\ -v $(pwd):/usr/src/app \\ python/pack \\ bash  This creates a container based on the image python/pack where you can accesss your project files from the directory /usr/src/app and then you can execute the code as you would do in your host machine. Lets break this command down line-by-line:\n  docker run \u0026ndash;rm -it : docker run tells the docker daemon that we want to create a new container, **-it **is where we run this container in the interactive mode. -i basically keeps the STDIN open and -t allocates a pseudo TTY. In simple words, -it is essential to open a bashshell inside the container, later. You can also use -d instead of -it which starts the container in detached mode, which starts the container in the background.\n  -u $(id -u):$(id -g) : It is not recommended to start a container as a root user. So we specify -u flag and then user ID $(id -u) and group ID $(id -g) to start a container as a regular user. Now we will have previliges over only that directory which is mounted to the container and not the entire container. This is the cleaner way of creating a container.\n  -v /etc/passwd:/etc/passwd: This -v argument mounts /etc/passwd of the host to /etc/passwd of the container. Otherwise the docker container can‚Äôt access the usernames and display I have no name! in the bash prompt. Mounting /etc/passwd solves this problem.\n  -v $(pwd):/usr/src/app: This -v argument mounts your project directory to /usr/src/app by doing which you can access your code in the project directory inside the docker container and the changes made inside the docker container will be available in the project directory (pwd prints the present working directory). So in simple words, this creates a shared space between the container and the host machine from where the host machine can access the packages and dependencies installed in the docker image.\n  python/pack: This is pretty straight-forward. This is where we mention, which image is the container will be based on. The resulting container will contain the architecture of the mentioned image.\n  bash: This is where we mention the command to be executed soon after the container is created. Since we want the bash prompt, we specify the command bash which opens a bash shell soon after the container is created.\n  By the end of this process, we will have a container which is built with the dependencies we need, sharing a same space as our project directory, totally isolated from out host machine. This is really useful since the dependencies are not installed in the host machine. The host machine only contains the project folder but it runs inside the customized container. Clean stuf !!\nExtras/Troubleshooting   Editing a Docker Image : Let‚Äôs say you have created a Docker image with 3 packages and now you want to install a new package to the already built image. You can do it by editing the imageDockerfile and running the same command docker build -t python/pack . and docker will make changes to the existing image from where the content of the Dockerfile has changed. To make these changes, it is important to note that Dockerfile is in the same folder as it was initially built. Because Docker daemon uses current Dockerfile folder as a build context and when changes to the Dockerfile is made, build context will be checked and from there the Image will be built.\n  Port Forwarding : Port forwarding is such a useful functionality inside docker container. Let‚Äôs say you are using Jupyter notebook inside your docker container and it exposes a port 8888. But it will be on the context of the container, and not on the host machine. So if you go to localhost:8888 in the host machine, it won‚Äôt work. So you map the host‚Äôs 8888 port to container‚Äôs 8888 port when creating the container and then you can access jupyter notebook by going to localhost:8888 on the host machine. You can map a port by adding -p 8888:8888 when creating the container where left side of the : is the port on the host and the right side is the port on the container.\n  Conclusion Docker is a revolutionary tech, there is no doubt about that. Once the developement is over, the developed project can be containerized and the container can be deployed in hosting platforms like Digital Ocean. Containers can be made to talk to each other and each container can be used as a microservice. The possibilities are endless. It depends on you how would use this tech to the fullest. Happy coding :)\nEssential Docker Commands  Pull Image from Dockerhub : docker pull \u0026lt;image name\u0026gt; Build a Docker Image : docker build -t \u0026lt;image name\u0026gt; \u0026lt;path\u0026gt; Create Docker Container : docker run [-it/-d] --rm -u \u0026lt;user id\u0026gt;:\u0026lt;group id\u0026gt; -v \u0026lt;host directory\u0026gt;:\u0026lt;container directory\u0026gt; -p \u0026lt;host port\u0026gt;:\u0026lt;container port\u0026gt; \u0026lt;image name\u0026gt; \u0026lt;initial command\u0026gt; List Running Container : docker ps List All Containers : docker ps -a List All Container ID\u0026rsquo;s : docker ps -aq Delete Single Container : docker rm \u0026lt;container id\u0026gt; Delete All Docker Containers : docker rm $(docker ps -aq) List all Images : docker images List all Image ID\u0026rsquo;s : docker images -aq Delete single image : docker rmi \u0026lt;image id\u0026gt; Delete all images : docker rmi $(docker images -aq) Start Interactive session with a container started in detached mode (-d) : docker exec -it \u0026lt;container_name\u0026gt; bash  Important Links   Installing Docker\n  Post Installation Steps\n  Dockerfile, full reference\n  Compose file reference\n  ","permalink":"https://0xv31an5a1i5.xyz/posts/docker-as-virtualenv/","summary":"Docker! Sounds fancy and simple but trust me it has revolutionized how the tech industry builds, ships and deploys the applications lately. Docker is basically a mini-operating system where you build and run your applications which is totally isolated from the native operating system in your computer, and the same container can be deployed in the hosting platforms. If you are new to docker and want to learn more about its intricacies, i highly encourage you to go through the links given in the end of this article to learn more about it.","title":"Docker, a replacement of Virtualenv for Python developement"},{"content":"Sounds Ridiculous! isn‚Äôt it ? Me comparing the Megabytes or Terabytes of senseless one‚Äôs and zero‚Äôs of code to something that is built with Lego bricks. Lego is supposed to be a fun process, right? But coding seems like spending too much time in front of the computer finding which semi-colon did you miss and where! And bang your head to the wall until you get it right. They‚Äôre nowhere close to a match. How is that even possible? Yeah! you might say that rather sarcastically, or had that notion in your mind all along. But trust me, this is a perfect analogy that I can put forth when it comes to the creative process of coding in its entirety. I call it Lego Coding. And it is something that you are already familiar with, but never acknowledged it for how amazing it really is.\nWhen I was 11 years old, I got my first real personal computer as my Birthday gift (The best birthday gift ever). Being a tech-geek, and being obsessed with pressing random keys in the keyboard for a mere fascination of seeing something on the monitor, coding was not my first intent. Moreover, I didn‚Äôt even know a process called ‚Äòcoding to makes wonderful things‚Äô existed back then. I, the kid, who was building castles with Lego bricks and breaking it to build new one‚Äôs, came across coding C and C++, few years later. And it caught my attention right away. So I did what most of the budding computer geeks and beginners to the art of coding do, you guessed it,\nprintf(‚ÄòHello My Computer, It‚Äôs your boi, Velan‚Äô);\n(I might have missed the semi colon in my first few attempts, but you get the point üòÇ) When I saw the output in the black screen, I was freaking out. Running around the house, shouting ‚ÄúMy Computer knows my name‚Äù. It sounded ridiculous, yet amazing, that you can teach your computer to do things, and it just obeys you without arguing! That is how I was introduced into the art of coding. And it didn‚Äôt seem like something that is a tedious process of finding missing semi-colons, (Sometimes I do feel like banging my head against the wall, Sometimes) But It seemed more like building things that I love, with Lego bricks that I was obsessed with, prior to coding. It made me feel curious and expressive. It made me feel like, I can do anything and everything using my creative intent and my computer. And that kept me going to build things with code.\nGrowing up, I took an academic path where there is a lot of coding and computer science involved, because that is what I was interested in. It was a place of ecstasy to me, where i‚Äôm officially experimenting with things that i love doing. But, to my surprise, I was astonished to see not everyone was engrossed into the process of coding as much as I was. And when my fellow student said ‚ÄúI don‚Äôt like coding!‚Äù, My obvious reply was, ‚ÄúAre you freaking kidding me? How can you even not love playing Lego with code?‚Äù. Gradually I realised that, not everyone was introduced to coding like I had been. Not everyone coded because they loved the process, or I could say, They were never enchanted by the thought of playing Lego with code. While I saw C, C++ and other tools as Lego bricks that I can build my castles with, few others saw it as an ‚ÄòAcademic-Getaway-Pass‚Äô That can get them into introductory interviews, by showcasing good marks. And it was a let down to see an Art getting wasted, By people turning into primary job seekers than turning into primary artists who can build things they love.\nNow i‚Äôm 21 years old and, I have been coding ever since. Playing Lego with code never gets old to me. And I can‚Äôt think of any other analogy that explains the process any better. I‚Äòm never frightened by errors popping up in my code (Trust me, I get so many of them), besides I get frightened when I don‚Äôt get one üòÇ. And coding has kept my childlike creativity and imagination going, and it has given me a new dimension to think about things that I would never have thought otherwise. I don‚Äôt code to save someone‚Äôs time or save this world from apocalypse or get people on mars, perhaps. I code because i‚Äôm enchanted by the mere process of coding as building something that I love, by keeping lines of code one on top of the other and create a system that is as beautiful as my Lego castle was. If my code goes on to save someone‚Äôs life or time, I‚Äôm glad it did. But my primary intent always remains to code because I love to do it. Nothing complicated. And I\u0026rsquo;ve never been tired of it, Nor will I ever be.\nIf you are someone who doesn‚Äôt like coding or someone who doesn‚Äôt know where to start coding from or if coding is not interesting to you anymore, Here‚Äôs my advice to you (I‚Äôm not a professional though. so, you have a total liberty to disagree with me üòä) Coding neither has a how-to-guide, nor a shortcut to be a coding sensei. Starting with coding is as simple as starting with it. It is an art. And art doesn‚Äôt come with guides. It is a process of learning through trial and error. If you‚Äôre coding, it is highly unlikely that you‚Äôll get things right on the first attempt, YOU DON‚ÄôT! Most of the people quit here saying, ‚ÄúI‚Äôm tired, Let me eat something‚Äù. But if you keep the process going, make errors, and learn from them to know where you went wrong, you will have a whole new appreciation for the process of coding as of how it shapes your logic and makes you think in a different light. And few years down the line you will probably sit in front of the computer just like me and write an article on how coding shaped your life, made you a better person, filled your life with creativity. Just in case you do, I‚Äôll be so happy to read through your story someday.\nSo unleash the art of coding as how you build things with Lego bricks. Write code, Make mistakes, understand the errors and solve them. When you were a kid, it‚Äôs highly unlikely that you build a beautiful castle in the first attempt. Similarly it‚Äôs highly unlikely that you‚Äôll build a world class software in your first attempt. You will gradually get better with time and practice. And it‚Äôs gonna involve a lot of error solving and experimenting. And believe me, when you make something built with code, that you can call your own, You will have a whole new appreciation for the efforts that you‚Äôve put to get there. And that‚Äòs what LEGO CODING does to you. It makes you be proud of yourself for the castle-like structure you have built with your Lego bricks of code. So, have fun writing code, have fun exploring fun new challenges in this world full of possibilities. Have a good day üòÑ\n","permalink":"https://0xv31an5a1i5.xyz/posts/lego-coding/","summary":"Sounds Ridiculous! isn‚Äôt it ? Me comparing the Megabytes or Terabytes of senseless one‚Äôs and zero‚Äôs of code to something that is built with Lego bricks. Lego is supposed to be a fun process, right? But coding seems like spending too much time in front of the computer finding which semi-colon did you miss and where! And bang your head to the wall until you get it right. They‚Äôre nowhere close to a match.","title":"Lego Coding"},{"content":"","permalink":"https://0xv31an5a1i5.xyz/elsewhere/","summary":"elsewhere","title":"Elsewhere"},{"content":"","permalink":"https://0xv31an5a1i5.xyz/uses/","summary":"uses","title":"Uses"}]